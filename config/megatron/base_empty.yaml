account_for_embedding_in_pipeline_split: false  # If set, *input* embedding layer will be treated as a standard transformerlayer in the context of partition and placement for pipeline parallelism.  # Comma-separated list of proportions for training, validation, and test split. For example the split `90,5,5` will use 90%% of data for training, 5%% for validation and 5%% for test.
account_for_loss_in_pipeline_split: false  # If set, loss layer will be treated as a standard transformerlayer in the context of partition and placement for pipeline parallelism.
accumulate_allreduce_grads_in_fp32: false  # Gradient accumulation and all-reduce in fp32.
adam_beta1: 0.9  # First coefficient for computing running averages of gradient and its square
adam_beta2: 0.999  # Second coefficient for computing running averages of gradient and its square
adam_eps: 1.0e-08  # Term added to the denominator to improvenumerical stability
add_bias_linear: true  # Disable bias in the linear layers
add_position_embedding: true  # Disable position embedding. Deprecated: use --position-embedding-type
add_qkv_bias: false  # Enable bias only in the QKV linear layers
adlr_autoresume: false  # Enable autoresume on adlr cluster.
adlr_autoresume_interval: 1000  # Intervals over which check for autoresumetermination signal
align_grad_reduce: true  # If not set, all PP stages will launch gradient reduces simultaneously. Otherwise, each PP stage will independently launch as needed.
align_param_gather: true  # If not set, all PP stages will launch param all-gathers simultaneously. Otherwise, each PP stage will independently launch as needed.
app_tag_run_name: null  # Jobs belonging to same training run, suppose to have the same name. It will be used to track progress of a training done over multiple different jobs
app_tag_run_version: 0.0.0  # The version of the training of which current job is part of. It will be used to track the changes in the application side which might change the performance baseline
apply_layernorm_1p: false  # Adjust LayerNorm weights such that they are centered around zero. This improves numerical stability.
apply_query_key_layer_scaling: false  # Scale Q * K^T by 1 / layer-number. Useful for fp16 training. Also sets `attention_softmax_in_fp32` to True.
apply_residual_connection_post_layernorm: false  # If set, use original BERT residula connection ordering.
apply_rope_fusion: true  # Disable rope fusion, the fusion is available only when using megatron-core.
async_save: null  # Output directory to save checkpoints to.  # Apply async checkpointing save. Currently works only with`torch_dist` distributed checkpoint format.
async_tensor_model_parallel_allreduce: true  # DEPRECATED. This flag is ignored.
attention_backend: auto  # choices: [<AttnBackend.flash: 1>, <AttnBackend.fused: 2>, <AttnBackend.unfused: 3>, <AttnBackend.local: 4>, <AttnBackend.auto: 5>],Attention backend to use (flash,fused,unfused,local,auto). Defaults to auto
attention_dropout: 0.1  # Post attention dropout probability.
attention_softmax_in_fp32: false  # Run attention masking and softmax in fp32.
auto_detect_ckpt_format: false  # Determine if the checkpoint format is in legacy or distributed format. If False, expects distributed checkpoint iff args.ckpt_format != "torch". Might slow down loading a bit (double rank0 ckpt load).  # choices: ['torch', 'torch_dist', 'zarr', 'torch_dcp'],Checkpoint format to use. torch is the format used by torch.save/load. torch_dist is a megatron built-in distributed checkpointing format. torch_dcp is the torch.distributed.checkpoint format.
barrier_with_L1_time: true  # If not set, use barrier with level 1 time measurements. Note that this is up to the user to make sure calling barrier with their timers will not result in hangs. This can happen if for example the user adds a level 1 timer that is not called by all ranks.
batch_size: null  # Old batch size parameter, do not use. Use --micro-batch-size instead
bert_binary_head: true  # Disable BERT binary head.
bert_embedder_type: megatron  # choices: ['megatron', 'huggingface'],Select either Megatron or Huggingface as the Bert embedder.
bert_load: null  # Directory containing a model checkpoint.  # Directory containing an BertModel checkpoint (needed to start ICT and REALM)
bf16: false  # Run model in bfloat16 mode.
bias_dropout_fusion: true  # Disable bias and dropout fusion.
bias_gelu_fusion: true  # Disable bias and gelu fusion.
bias_swiglu_fusion: true  # Disable bias and swiglu fusion, the fusion is available only when using megatron-core.
biencoder_projection_dim: 0  # Size of projection head used in biencoder (paper default: 128)
biencoder_shared_query_context_model: false  # Whether to share the parameters of the query and context models or not
block_data_path: null  # The weight and prefix list for a set of train, validation, and testdatasets which split according to --split. The accepted formats are: (1) a single prefix, (2) a list of weight prefix pairs e.g. weight1 prefix1 weight2 prefix2, (3) a list of prefixes e.g. prefix1 prefix2. For (3), weights are inferred from the lengths of the contributing datasets. This argument is exclusive to the other independent --*-data-path arguments.  # Where to save/load BlockData to/from
calc_ft_timeouts: false  # If set, FT package will try to automatically compute the timeouts. Note: This feature is for Nvidia internal use only.
calculate_per_token_loss: false  # Scale cross entropy loss by the number of non-padded tokens in the global batch, versus the default behavior of assuming all tokens are non-padded.
check_for_large_grads: false  # Check for unexpectedly large grads
check_for_nan_in_loss_and_grad: true  # Check for NaNs in loss and grad
check_for_spiky_loss: false  # Check for spiky loss
check_weight_hash_across_dp_replicas_interval: null  # Interval to check weight hashes are same across DP replicas. If not specified, weight hashes not checked.
checkpoint_activations: false  # Checkpoint activation to allow for training with larger models, sequences, and batch sizes.
ckpt_assume_constant_structure: false  # If the model and optimizer state dict structure isconstant throughout a *single training job*, it allows fordifferent checkpointing performance optimizations.
ckpt_convert_format: null  # choices: ['torch', 'torch_dist', 'zarr'],Checkpoint format for conversion.
ckpt_convert_save: null  # Save directory for converted checkpoint.
ckpt_convert_update_legacy_dist_opt_format: false  # When loading a checkpoint, update the legacy format for the distributed optimizer, which previously used a merged param/grad buffer and a different bucket mapping. The legacy format was deprecated on Feb 13, 2024.
ckpt_format: torch_dist
ckpt_fully_parallel_load: false  # Apply full load parallelization across DP for distributed checkpoints.
ckpt_fully_parallel_save: true  # Disable applying full save parallelization across DP for distributed checkpoints. Depending on ckpt format might decrease the number of files in the checkpoint. Makes DistributedOptimizer checkpoint non-reshardable.
ckpt_fully_parallel_save_deprecated: false  # Deprecated: see --no-ckpt-fully-parallel-save.
ckpt_step: null  # Checkpoint step to load model from.
classes_fraction: 1.0  # training with fraction of classes.
clip_grad: 1.0  # Gradient clipping based on global L2 norm.
clone_scatter_output_in_embedding: true  # If not set, clone the output of the scatter in embedding layer to GC original tensor.
config_logger_dir: ''  # If set, will dump all configs to --config-logger-dir
context_parallel_size: 1  # Degree of context parallelism.
cp_comm_type:  # Inter-gpu communication type for context parallelism: p2p, a2a, allgather or a2a+p2p. If a single string is provided, all layers will share the same communication type. Users can also specify separated types for each layer like --cp-comm-type p2p p2p a2a a2a a2a+p2p a2a+p2p
- p2p
create_attention_mask_in_dataloader: true  # If set, do not create attention_masks in dataloader.
cross_entropy_fusion_impl: native  # choices: ['native', 'te'],Implementation of cross entropy loss calculation.
cross_entropy_loss_fusion: false  # Enabled fusion of cross entropy loss calculation.
cuda_graph_scope: full  # choices: ['full', 'attn'],Determines the CUDA graphs capturing scope. Valid values are "full" and "attn". "Full" scope captures a whole Transformer layer. "Attn" scope only captures operations in TransformerLayer._forward_attention().
cuda_graph_warmup_steps: 3  # Number of CUDA graph warmup steps
data_args_path: null  # Path to data-args. Instead of feeding `--data-path` with weighted dataset, we pass in a file path from which we read that argument. This is useful when the list of data is too big.
data_cache_path: null  # Path to a directory to hold cached index files.
data_parallel_random_init: false  # Enable random initialization of params across data parallel ranks
data_parallel_sharding_strategy: no_shard  # choices: ['no_shard', 'optim', 'optim_grads', 'optim_grads_params'],Sharding strategy of data parallelism.
data_path: null
data_per_class_fraction: 1.0  # training with fraction of data per class.
data_sharding: true  # Disable data sharding.
dataloader_type: null  # choices: ['single', 'cyclic', 'external'],Single pass vs multiple pass data loader
ddp_average_in_collective: false  # If set, average directly in data-parallel communication collective.
ddp_bucket_size: null  # Bucket size for data-parallel communication
ddp_num_buckets: null  # Number of buckets for data-parallel communication
ddp_pad_buckets_for_high_nccl_busbw: false  # If set, make sure the bucket size is divisible by a large power of 2 (2^16) to ensure NCCL collectives have high bus bandwidth at large DP counts, since NCCL message size (which for ring algorithms is bucket_size / dp_size) apparently needs to be divisible by a power of 2 for high busbw.
decoder_first_pipeline_num_layers: null  # Number of transformer layers.  # The number of transformer layers on the first pipeline stage of the decoder. Default None is even split of transformer layers across all pipeline stages
decoder_last_pipeline_num_layers: null  # The number of transformer layers on the last pipeline stage of the decoder. Default None is even split of transformer layers across all pipeline stages
decoder_num_layers: null  # Number of decoder transformer layers.
decoder_seq_length: null  # Maximum sequence length to process.  # Maximum decoder sequence length to process.
decoupled_lr: null  # Initial learning rate. Depending on decay style and initial warmup, the learning rate at each iteration would be different.  # Separate learning rate for the input and output layer
decoupled_min_lr: null  # Minimum value for learning rate. The schedulerclip values below this threshold.  # Minimum value for learning rate for the input and output layer. The schedulerclip values below this threshold
decrease_batch_size_if_needed: false  # If set, decrease batch size if microbatch_size * dp_sizedoes not divide batch_size. Useful for KSO (Keep Soldiering On)to continue making progress if number of healthy GPUs (andcorresponding dp_size) does not support current batch_size.Old batch_size will be restored if training is re-started withdp_size that divides batch_size // microbatch_size.
defer_embedding_wgrad_compute: false  # If set, defers the vocabulary projection linear layer weightgradient compute to pipeline flush.
delay_wgrad_compute: false  # Delay the wgrad compute for batch-level overlapping
deprecated_use_mcore_models: false  # DEPRECATED. Use the implementation from megatron core.Now ignored and mcore models are the default, use --use-legacy-models to not use core models.
deterministic_mode: false  # Choose code that has deterministic execution. This usually means slower execution, but is good for debugging and testing.
dino_bottleneck_size: 256  # Bottle neck dimension in dino head 
dino_freeze_last_layer: 1  # Freezing last layer weights
dino_head_hidden_size: 2048  # Tansformer hidden size.  # Hidden dimension size in dino head
dino_local_crops_number: 10  # Number of local crops
dino_local_img_size: 96  # Image size for vision classification task
dino_norm_last_layer: false  # Disable Norm in last layer.
dino_teacher_temp: 0.07  # teacher temperature
dino_warmup_teacher_temp: 0.04  # warump teacher temperature
dino_warmup_teacher_temp_epochs: 30  # warmup teacher temperaure epochs
disable_bf16_reduced_precision_matmul: false  # If True, sets torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction=False to prevent matmul from using reduced precision accumulation when using BF16.
disable_mamba_mem_eff_path: false  # Disable Mamba efficient path.
disable_straggler_on_startup: false  # If set, StragglerDetector is disabled on startup.
dist_ckpt_format_deprecated: null  # Deprecated: see --ckpt-format.
dist_ckpt_strictness: assume_ok_unexpected  # choices: ['assume_ok_unexpected', 'log_unexpected', 'log_all', 'raise_unexpected', 'raise_all', 'return_unexpected', 'return_all', 'ignore_all'],Determine handling of key mismatch during checkpoint load. Check StrictHandling docs for flags meaning. NOTE: This flag controls only distributed checkpoint load from storage, not loading state dict into the model.
distribute_saved_activations: false  # If set, distribute recomputed activations across model parallel group.
distributed_backend: nccl  # choices: ['nccl', 'gloo'],Which backend to use for distributed training.
distributed_timeout_minutes: 10  # Timeout minutes for torch.distributed.
embedding_path: null  # Where to save/load Open-Retrieval Embedding data to/from
empty_unused_memory_level: 0  # choices: [0, 1, 2],Call torch.cuda.empty_cache() each iteration (training and eval), to reduce fragmentation.0=off, 1=moderate, 2=aggressive.
enable_cuda_graph: false  # Use CUDA graph capture and replay.
enable_experimental: false  # Enable experimental features.
enable_ft_package: false  # If set, Fault Tolerance package is enabled. Note: This feature is for Nvidia internal use only.
enable_gloo_process_groups: true  # Disables creation and usage of Gloo process groups.
enable_msc: true  # Disable the usage of Multi-Storage Client (MSC) in Megatron Core.
enable_one_logger: true  # If set, disable using one_logger to track E2E metricsNote that one_logger is an internal tool and not available externally. For installation, please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositoriesfor more details
encoder_num_layers: null  # Number of encoder transformer layers.
encoder_pipeline_model_parallel_size: 0  # Degree of pipeline model parallelism.  # DEPRECATED (will be removed in core_r0.14.0): Use orthotope parallelism management instead. Degree of pipeline model parallelism in the encoder. This is independent of the amount of pipeline in the decoder.  # Old model parallel argument, do not use. Use --tensor-model-parallel-size instead.
encoder_seq_length: null  # Maximum encoder sequence length to process.This should be exclusive of --seq-length
encoder_tensor_model_parallel_size: 0  # Degree of tensor model parallelism.  # DEPRECATED (will be removed in core_r0.14.0): Use orthotope parallelism management instead. Degree of tensor model parallelism for the encoder.
end_weight_decay: null  # Weight decay coefficient for L2 regularization.  # End of run weight decay coefficient for L2 regularization.
eod_mask_loss: false  # Mask loss for the end of document tokens.
error_injection_rate: 0  # Rate at which to inject unexpected results, e.g. 1000 means once every 1000 result validations
error_injection_type: transient_error  # choices: ['correct_result', 'transient_error', 'persistent_error'],Type of error to inject. 
eval_interval: 1000  # Interval between running evaluation on validation set.
eval_iters: 100  # Number of iterations to run for evaluationvalidation/test for.
evidence_data_path: null  # Path to Wikipedia Evidence frm DPR paper
exit_duration_in_mins: null  # Exit the program after this many minutes.
exit_interval: null  # Exit the program after the iteration is divisible by this value.
exit_on_missing_checkpoint: false  # If '--load' is set, but checkpoint is not found (e.g., path typo), then exit instead of random initialization.
exit_signal_handler: false  # Dynamically save the checkpoint and shutdown the training if SIGTERM is received
exp_avg_dtype: fp32  # choices: ['fp32', 'fp16', 'bf16', 'fp8'],Dtype of exp_avg (1st moment in adam optimizer) when enabling precision-aware-optimizer. This dtype is used for storing the optimizer state in memory during training but does not affect the precision in the kernel computation.
exp_avg_sq_dtype: fp32  # choices: ['fp32', 'fp16', 'bf16', 'fp8'],Dtype of exp_avg_sq (2nd moment in adam optimizer) when enabling precision-aware-optimizer. This dtype is used for storing the optimizer state in memory during training but does not affect the precision in the kernel computation.
expert_model_parallel_size: 1  # Degree of expert model parallelism.
expert_tensor_parallel_size: null  # Degree of expert model parallelism. Default is None, which will be set to the value of --tensor-model-paralle-size.
external_cuda_graph: false  # Use CUDA graph capture and replay. The CUDA graphs aremanually captured in the training script.
ffn_hidden_size: null  # Transformer Feed-Forward Network hidden size. This is set to 4*hidden-size if not provided
finetune: false  # Load model for finetuning. Do not load optimizer or rng state from checkpoint and set iteration to 0. Assumed when loading a release checkpoint.
first_last_layers_bf16: false  # Construct first and last layers in bf16 when doing FP8 training.
flash_decode: false  # Whether to use the flash decoding kernel.
fp16: false  # Run model in fp16 mode.
fp16_lm_cross_entropy: false  # Move the cross entropy unreduced loss calculationfor lm head to fp16.
fp32_residual_connection: false  # Move residual connections to fp32.
fp8: null  # choices: ['e4m3', 'hybrid'],Which fp8 format scheme to use for FP8 tensors in the forward and backward pass
fp8_amax_compute_algo: most_recent  # choices: ['most_recent', 'max'],Algorithm for computing amax from history
fp8_amax_history_len: 1  # Number of steps for which amax history is recorded per tensor
fp8_interval: 1  # DEPRECATED. This flag is ignored. Scaling update interval for fp8
fp8_margin: 0  # Scaling margin for fp8
fp8_param_gather: false  # Keep the compute param in fp8 (do not use any other intermediate dtype) and perform the param all-gather in fp8.
fp8_recipe: delayed  # choices: ['tensorwise', 'delayed', 'mxfp8', 'blockwise'],Which fp8 recipe to use for FP8 tensors in the forward and backward pass
fp8_wgrad: true  # Execute wgrad in higher precision even for FP8 runs
fsdp_double_buffer: false  # Enable double buffering for temporary memory needed for custom FSDP communications. Double-buffering the communication memory improves memory management efficiency by reusing previously allocated buffers, rather than creating new buffers for each FSDP communication. This is required for user buffer registration and is enabled by default when using NCCL user buffers.
global_batch_size: null  # Training batch size. If set, it should be a multiple of micro-batch-size times data-parallel-size. If this value is None, then use micro-batch-size * data-parallel-size as the global batch size. This choice will result in 1 for number of micro-batches.
grad_reduce_in_bf16: false  # Reduce gradients in bfloat16.
gradient_accumulation_fusion: true  # Disable fusing gradient accumulation to weight gradient computation of linear layers
gradient_reduce_div_fusion: true  # If not set, fuse the division in gradient reduce.
group_query_attention: false  # Use group-query attention.
head_lr_mult: 1.0  # learning rate multiplier for head during finetuning
heterogeneous_layers_config_encoded_json: null  # This is encoded json string of the heterogeneous model configuration. Used to keep the content of the heterogeneous model specification in args when the model is loaded from a checkpoint. Use the format of the HuggingFace config files in llama nemotron models, e.g. https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1/resolve/main/config.json.
heterogeneous_layers_config_path: null  # Path to json file containing heterogeneous model configuration. Use the format of the HuggingFace config files in llama nemotron models, e.g. https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1/resolve/main/config.json.
hidden_dropout: 0.1  # Dropout probability for hidden state transformer.
hidden_size: null
hierarchical_context_parallel_sizes: null  # Degrees of the hierarchical context parallelism. Users should provide a list to specify the sizes for different levels. --hierarchical-context-parallel-sizes 2 4 indicates every two adjacent gpus forms the first level of cp groups and the cp ranks with the same odevity forms the second level of cp groups.
high_priority_stream_groups: []  # The communicator group names to use high priority streams.
hybrid_attention_ratio: 0.0  # Ratio of attention layers to total layers, in the range [0.0, 1.0].
hybrid_mlp_ratio: 0.0  # Ratio of mlp layers to total layers, in the range [0.0, 1.0].
hybrid_override_pattern: null  # Force a specific hybrid layer pattern. The valueshould be a string of characters chosen fromcore.ssm.mamba_hybrid_layer_allocation.Symbols.If a value greater than 0.0 is supplied to any of the hybrid ratio arguments, then the number of each typeof layer in the override pattern must match number inthe overidden pattern
hysteresis: 2  # hysteresis for dynamic loss scaling
ict_head_size: null  # Size of block embeddings to be used in ICT and REALM (paper default: 128)
ict_load: null  # Directory containing an ICTBertModel checkpoint
img_h: 224  # Image height for vision classification task
img_w: 224  # Image height for vision classification task
indexer_batch_size: 128  # How large of batches to use when doing indexing jobs
indexer_log_interval: 1000  # Report loss and timing interval.  # After how many batches should the indexer report progress
inference_batch_times_seqlen_threshold: -1  # If (batch-size * sequence-length) is smaller than this thresholdthen batches will not be split up for pipelining.Requires setting --pipeline-model-parallel-size > 1.Setting this to -1 indicates that batch pipelining is not used.
inference_dynamic_batching: false  # Enable dynamic batching mode.
inference_dynamic_batching_buffer_guaranteed_fraction: 0.2  # Space is reserved within the inference context memory buffer to guarantee that a minimum number of active requests will always be able to run to completion. This is to avoid the context being blocked by paused requests.
inference_dynamic_batching_buffer_overflow_factor: null  # Scaling factor over the memory buffer size for auto computing `max_requests` and `max_tokens`. This scaling factor is used for fitting more requests and tokens in the memory buffer than it can safely hold, which in turn increases throughput.
inference_dynamic_batching_buffer_size_gb: 40.0  # Total buffer size (GB) allocated for the chunked KV memory.
inference_dynamic_batching_chunk_size: 256  # KV cache chunk size. It should be a multiple of 256
inference_dynamic_batching_max_requests_override: null  # If set, this overrides the max requests as computed from `--inference-dynamic-batching-buffer-overflow-factor`.
inference_dynamic_batching_max_tokens_override: null  # If set, this overrides the max tokens as computed from `--inference-dynamic-batching-buffer-overflow-factor`.
inference_max_batch_size: 8  # Maximum number of requests for inference.
inference_max_seq_length: 2560  # Maximum sequence length expected for inference (prefill + decode).
inference_rng_tracker: false  # Use a random number generator configured for inference.
init_method_std: 0.02  # Standard deviation of the zero mean normal distribution used for weight initialization.
init_method_xavier_uniform: false  # Enable Xavier uniform parameter initialization
init_model_with_meta_device: false
initial_loss_scale: 4294967296  # Static loss scaling, positive power of 2 values can improve fp16 convergence. If None, dynamicloss scaling is used.  # Initial loss-scale for dynamic loss scaling.
inprocess_active_world_size: 1  # The number of ranks initially executing the workload. The remaining ranks from the allocation are set aside as warm reserve.
inprocess_barrier_timeout: 120  # Timeout (in seconds) for internal distributed barrier
inprocess_completion_timeout: 120  # Timeout (in seconds) for barrier on completion on all ranks
inprocess_empty_cuda_cache: false  # Release all unoccupied cached GPU memory on every in-process restart.
inprocess_granularity: node  # choices: ['node', 'rank'],Granularity for in-process restart.
inprocess_hard_timeout: 90  # Hard progress timeout (in seconds).
inprocess_heartbeat_interval: 30  # Monitoring interval (in seconds) for detecting unresponsive ranks.
inprocess_heartbeat_timeout: 60  # Timeout (in seconds) for a missing rank detection heartbeat.
inprocess_last_call_wait: 1  # Time interval (in seconds) for other ranks to report concurrent terminal failures.
inprocess_max_iterations: null  # Maximum number of in-process restart iterations.
inprocess_monitor_process_interval: 1.0  # Monitoring interval (in seconds) for the monitoring process.
inprocess_monitor_thread_interval: 1.0  # Monitoring interval (in seconds) for the monitoring thread.
inprocess_progress_watchdog_interval: 1.0  # Interval (in seconds) for automatic progress watchdog timestamp updates.
inprocess_restart: false  # Enables in-process restart.
inprocess_soft_timeout: 60  # Soft progress timeout (in seconds).
inprocess_termination_grace_time: 1  # Interval (in seconds) between SIGTERM and SIGKILL issued on hard timeout
is_hybrid_model: false  # Indicates whether the model is a hybrid model.
iter_per_epoch: 1250  # iterations per epoch
iterations_to_skip: []  # List of iterations to skip, empty by default.
keep_fp8_transpose_cache_when_using_custom_fsdp: false  # If set, keep the fp8 transpose cache when using custom FSDP.
kitchen_config_file: null  # Use the config .yaml file at the specified location to configure kitchen quantization.
kitchen_recipe_number: null  # Use a default kitchen recipe for all layers as defined by QAT_PARAMS index
kv_channels: null  # Projection weights dimension in multi-head attention. This is set to    args.hidden_size // args.num_attention_heads if not provided.
kv_lora_rank: 32  # Rank of Key and Value tensors' low rank representation.
lazy_mpu_init: null  # If set to True, initialize_megatron() skips DDP initialization and returns function to complete it instead. Also turns on --use-cpu-initialization flag. This is for external DDP manager.
load: null
load_model_opt_format: false  # Load a checkpoint for TensorRT model optimizer (nvidia-modelopt).This function can also be used to load NeMo .nemo sharded checkpoints.
local_rank: 0  # local rank passed from distributed launcher.
log_energy: false  # If set, log energy consumption (in Joules)
log_interval: 100
log_loss_scale_to_tensorboard: true  # Disable loss-scale logging to tensorboard.
log_memory_to_tensorboard: false  # Enable memory logging to tensorboard.
log_num_zeros_in_grad: false  # If set, calculate and log the number of zeros in gradient.
log_params_norm: false  # If set, calculate and log parameters norm.
log_progress: false  # If set, log progress (in terms of number of processed tokens and number of floating-point operations) to progress.txt file in checkpoint directory.
log_straggler: false  # If set, tracks and logs straggler per GPU.
log_throughput: false  # If set, calculate and log throughput per GPU.
log_timers_to_tensorboard: false  # If set, write timers to tensorboard.
log_validation_ppl_to_tensorboard: false  # If set, write validation perplexity to tensorboard.
log_world_size_to_tensorboard: false  # Enable world size logging to tensorboard.
logging_level: null  # Set default logging level
loss_scale: null
loss_scale_window: 1000  # Window over which to raise/lower dynamic scale.
lr: null
lr_decay_iters: null  # number of iterations to decay learning rate over, If None defaults to `--train-iters`
lr_decay_samples: null  # number of samples to decay learning rate over, If None defaults to `--train-samples`
lr_decay_style: linear  # choices: ['constant', 'linear', 'cosine', 'inverse-square-root', 'WSD'],Learning rate decay function.
lr_warmup_fraction: null  # fraction of lr-warmup-(iters/samples) to use for warmup (as a float)
lr_warmup_init: 0.0  # Initial value for learning rate warmup. The scheduler starts warmup from this value.
lr_warmup_iters: 0  # number of iterations to linearly warmup learning rate over.
lr_warmup_samples: 0  # number of samples to linearly warmup learning rate over.
lr_wsd_decay_iters: null  # number of iterations for the annealing phase in the wsd schedule
lr_wsd_decay_samples: null  # number of samples for the annealing phase in the wsd schedule
lr_wsd_decay_style: exponential  # choices: ['exponential', 'linear', 'cosine', 'minus_sqrt'],Decay style for the annealing phase of WSD
main_grads_dtype: fp32  # choices: ['fp32', 'bf16'],Dtype of main grads when enabling precision-aware-optimizer
main_params_dtype: fp32  # choices: ['fp32', 'fp16'],Dtype of main params when enabling precision-aware-optimizer
make_vocab_size_divisible_by: 128  # Pad the vocab size to be divisible by this value.This is added for computational efficieny reasons.
mamba_head_dim: 64  # Head dimension for Mamba layers.
mamba_num_groups: 8  # Number of groups for Mamba layers.
mamba_num_heads: null  # Number of heads for Mamba layers.If not set, then the number of heads will be --hidden-size * expand // --mamba-head-dim
mamba_state_dim: 128  # State dimension for Mamba layers.
manual_gc: false  # Disable the threshold-based default garbage collector and trigger the garbage collection manually. Manual garbage collection helps to align the timing of the collection across ranks which mitigates the impact of CPU-associated jitters. When the manual gc is enabled, garbage collection is performed only at the start and the end of the validation routine by default.
manual_gc_eval: true  # When using manual garbage collection, disable garbage collection at the start and the end of each evaluation run.
manual_gc_interval: 0  # Training step interval to trigger manual garbage collection. When the value is set to 0, garbage collection is not triggered between training steps.
mask_factor: 1.0  # mask size scaling parameter
mask_prob: 0.15  # Probability of replacing a token with mask.
mask_type: random  # choices: ['random', 'row'],mask types
masked_softmax_fusion: true  # Disable fusion of query_key_value scaling, masking, and softmax.
max_position_embeddings: null  # Maximum number of position embeddings to use. This is the size of position embedding.
max_tokens_to_oom: 12000  # Maximum number of tokens during inferencetokens here is # in prompt + # to generateAllows us to throw an error before OOM crashes server
memory_snapshot_path: snapshot.pickle  # Specifies where to dump the memory history pickle.
merge_file: null  # Path to the BPE merge file.
micro_batch_size: null  # Batch size per model instance (local batch size). Global batch size is local batch size times data parallel size times number of micro batches.
microbatch_group_size_per_vp_stage: null  # Number of contiguous microbatches per virtual pipeline stage
mid_level_dataset_surplus: 0.005  # The sample surplus to build for the mid-level datasets(s)
min_loss_scale: 1.0  # Minimum loss scale for dynamic loss scaling.
min_lr: 0.0
mlp_chunks_for_prefill: 1  # Number of chunks along sequence dimension for MLP computation during prefill
mmap_bin_files: true  # Disable mmap-ing of .bin files.
mock_data: false  # Skip data loading and validation and opt for artificial generation of mock data when an implementation is available.
model_parallel_size: null
moe_apply_probs_on_input: false  # Apply probs before mlp activation for moe routing.
moe_aux_loss_coeff: 0.0  # Scaling coefficient for the aux loss: a starting value of 1e-2 is recommended.
moe_deepep_num_sms: 20  # Number of SMs to use for DeepEP.
moe_enable_deepep: false  # [Experimental] Enable DeepSeek/DeepEP for efficient token dispatching and combine in MoE models. Only works with flex token dispatcher by setting --moe-token-dispatcher-type=flex.
moe_expert_capacity_factor: null  # The capacity factor for each expert, None means no token will be dropped.
moe_extended_tp: false  # Deprecated. Use --expert-tensor-parallel-size instead.
moe_ffn_hidden_size: null  # The hidden size of each expert's feed-forward network (ffn). If not specified, defaults to the ffn_hidden_size.
moe_grouped_gemm: false  # When there are multiple experts per rank, launch multiple local GEMM kernels in multiple streams to improve the utilization and performance with GroupedLinear in TransformerEngine.
moe_input_jitter_eps: null  # Add noise to the input tensor by applying jitter with a specified epsilon value.
moe_layer_freq: 1  # Frequency between MoE layers and Dense layers. Accepts either: - An integer N: Represents a 1:N ratio, meaning one expert layer for every N-1 dense layers - A string containing a Python list expression that defines a custom pattern, e.g.: "([1]*3+[0]*1)*3" evaluates to [1,1,1,0,1,1,1,0,1,1,1,0] where 1 indicates an expert layer and 0 indicates a dense layer. Examples: "([0]+[1]*23)": 1 dense layer followed by 23 experts layers, "([1]*3+[0]*2)*2": Three expert layers followed by two dense layers, repeated twice.
moe_layer_recompute: false  # Enable checkpointing for moe_layer, should be used when memory is not sufficient. Deprecated. Use "--recompute-granularity selective --recompute-modules moe" instead.
moe_pad_expert_input_to_capacity: false  # Pads the input for each expert to match the expert capacity length, effective only after the --moe-expert-capacity-factor is set.
moe_per_layer_logging: false  # Enable per-layer logging for MoE, currently supports auxiliary loss and z loss.
moe_permute_fusion: false  # Fuse token rearrangement ops during token dispatching.
moe_router_bias_update_rate: 0.001  # Expert bias update rate in the aux-loss-free load balancing strategy. The expert bias is updated based on the number of assigned tokens to each expert in a global batch, where the bias is increased for the experts with less assigned tokens and decreased for the experts with more assigned tokens. The default value 1e-3 is same as that used in DeepSeekV3.
moe_router_dtype: null  # choices: ['fp32', 'fp64'],Data type for routing computation and expert output weighted averaging. Fp32/fp64 enhances numerical stability, especially with numerous experts. The perf impact should be negligible when used with permute fusion. None means no changes for dtype.
moe_router_enable_expert_bias: false  # TopK routing with dynamic expert bias in the aux-loss-free load balancing strategy. The routing decision is based on the sum of the routing scores and the expert bias. See https://arxiv.org/abs/2408.15664 for details.
moe_router_force_load_balancing: false  # [Experimental] Force override routing to balance token distribution using random logits for MoE routers, supporting naive top-k and group-limited top-k. This experimental feature is for benchmarking purposes only!
moe_router_group_topk: null  # Number of selected groups for group-limited routing.
moe_router_load_balancing_type: aux_loss  # choices: ['aux_loss', 'seq_aux_loss', 'sinkhorn', 'none'],Determines the load balancing strategy for the router. "aux_loss" corresponds to the load balancing loss used in GShard and SwitchTransformer; "seq_aux_loss" corresponds to the load balancing loss used in DeepSeekV2, which computes the loss for each individual sample; "sinkhorn" corresponds to the balancing algorithm used in S-BASE, and "none" implies no load balancing. The default is "aux_loss".
moe_router_num_groups: null  # Number of groups to divide experts into for group-limited routing. When using group-limited routing: 1) Experts are divided into equal-sized groups, 2) For each token, a subset of groups are selected based on routing scores (sum of top-2 expert scores within each group), 3) From these selected groups, moe_router_topk experts are chosen.Two common use cases: 1) Device-limited routing: Set equal to expert parallel size (EP) to limit each token to experts on a subset of devices (See DeepSeek-V2: https://arxiv.org/pdf/2405.04434) 2) Node-limited routing: Set equal to number of nodes in EP group to limit each token to experts on a subset of nodes (See DeepSeek-V3: https://arxiv.org/pdf/2412.19437)
moe_router_padding_for_fp8: false  # Pad the routing_map to make sure the number of tokens each expert received is a multiple of 16/32 for FP8 precision. It is suggested to enable this for dropless training with FP8 precision when num_local_experts > 1. This is a more efficient way to pad for FP8 which eliminates the explicit padding in the GroupedMLP layer.
moe_router_pre_softmax: false  # Enable pre-softmax routing for MoE, which means softmax is before the top-k selection. By default, softmax is done after top-k.
moe_router_score_function: softmax  # choices: ['softmax', 'sigmoid'],Score function for MoE TopK routing. Can be "softmax" or "sigmoid".
moe_router_topk: 2  # Number of experts to route to for each token. The default is 2.
moe_router_topk_scaling_factor: null  # Scaling factor for routing score in top-k selection, only works when --moe-router-pre-softmax enabled. Defaults to None, which means no scaling.
moe_shared_expert_intermediate_size: null  # Shared expert total ffn hidden size. It should be equal to "num_shared_experts * ffn_size_of_each_shared_expert" if there are multiple shared experts. None means no shared expert.
moe_shared_expert_overlap: false  # Enable overlapping between shared expert computations and dispatcher communications. Without this, the shared epxerts execute after the routed experts. Only effective when moe-shared-expert-intermediate-size is set.
moe_token_dispatcher_type: allgather  # choices: ['allgather', 'alltoall', 'flex'],The type of token dispatcher to use. The default is 'allgather'. Options are 'allgather', 'alltoall'. We recommend using 'alltoall' when applying expert parallelism. For more information, please refer to the documentation in core/moe/README.
moe_token_drop_policy: probs  # choices: ['probs', 'position'],The policy to drop tokens. Can be either "probs" or "position". If "probs", the tokens with the lowest probabilities will be dropped. If "position", tokens at the end of each batch will be dropped.
moe_upcycling_granularity: 1  # This param sepecifics how many times smaller is the expert hidden size compared with the original dense FFN hidden size. For using granular upcycling strategy, please set this param as a positive integer. If this param is set to 1, it means using the default upcycling strategy.
moe_use_legacy_grouped_gemm: false  # Use legacy GroupedMLP rather than TEGroupedMLP. Note: The legacy one will be deprecated soon.
moe_use_upcycling: false  # Load a checkpoint of a dense model, convert it into an MoE model, and save the converted model to the path specified by --save. Upcycling is implemented on the top of distributed checkpointing, so it supports parallel modes different from the dense model.
moe_z_loss_coeff: null  # Scaling coefficient for the z-loss: a starting value of 1e-3 is recommended.
mrope_section: null  # Multimodal rope section is for channel dimension, empty by default.
mscale: 1.0  # Mscale for YaRN RoPE in multi-latent attention.
mscale_all_dim: 1.0  # Mscale all dimensions for YaRN RoPE in multi-latent attention.
mtp_loss_scaling_factor: 0.1  # Scaling factor of Multi-Token Prediction (MTP) loss. We compute the average of the MTP losses across all depths, and multiply it the scaling factor to obtain the overall MTP loss, which serves as an additional training objective.
mtp_num_layers: null  # Number of Multi-Token Prediction (MTP) Layers.MTP extends the prediction scope to multiple future tokens at each position.This MTP implementation sequentially predict additional tokens by using D sequential modules to predict D additional tokens.
multi_latent_attention: false  # Use multi-latent attention for model.
nccl_all_reduce_for_prefill: false  # When using symmeric all reduce kernels this will use regular nccl kernels for prefill. This can be more effecient when prefill is large as the nccl kernels can be more bandwith optimized
nccl_communicator_config_path: null  # Path to the yaml file with NCCL communicator configurations. The number of min/max thread groups and thread group cluster size of each communicator can be configured by setting `min_ctas`, `max_ctas`, and `cga_cluster_size`.
nccl_ub: false  # Use the userbuffer registration for DP/FSDP communication buffers.This option will reduce GPU SM usage for the DP/FSDP communication,which is improving the performance of the overlapped computation.
no_load_optim: null  # Do not load optimizer when loading checkpoint.
no_load_rng: null  # Do not load rng state when loading checkpoint.
no_persist_layer_norm: false  # Disable using persistent fused layer norm kernel. This kernel supports only a set of hidden sizes. Please check persist_ln_hidden_sizes if your hidden size is supported.
no_rope_freq: null  # Controls which layers to skip performing Rotary Position Embedding. Accepts either: - An integer N: Represents a 1:N ratio, meaning RoPE is skipped every N-1 layers. - A string containing a Python list expression that defines a custom pattern, e.g.: "([0]*3+[1]*1)*3" evaluates to [0,0,0,1,0,0,0,1,0,0,0,1] where 1 indicates no-rope layer. This patten is equivalent to --no-rope-freq=4.By default this is disabled and set to None, indicating RoPE will be performedon every layer.
no_save_optim: null  # Do not save current optimizer.
no_save_rng: null  # Do not save current rng state.
non_persistent_ckpt_type: null  # choices: ['global', 'local', 'in_memory', None],Type of non-persistent model checkpoints. "global" - Saved as a standard checkpoint (e.g., on Lustre) with old checkpoints being removed. "local" - Each rank saves a portion of the checkpoint locally (e.g., on SSD/ramdisk). None - No non-persistent checkpointing (default option).
non_persistent_global_ckpt_dir: null  # Directory containing global non-persistent model checkpoints.
non_persistent_local_ckpt_algo: fully_parallel  # choices: ['fully_parallel', 'atomic'],Algorithm for local non-persistent checkpointing.
non_persistent_local_ckpt_dir: null  # Directory containing local non-persistent model checkpoints.
non_persistent_save_interval: null  # Number of iterations between persistent checkpoint saves.  # Number of iterations between non-persistent saves.
norm_epsilon: 1.0e-05  # Epsilon for layer norm and RMS norm.
normalization: LayerNorm  # choices: ['LayerNorm', 'RMSNorm'],Which normalization technique to use.
num_attention_heads: null  # Number of transformer attention heads.
num_channels: 3  # Number of channels in input image data
num_classes: 1000  # num of classes in vision classificaiton task
num_dataset_builder_threads: 1  # Number of parallel threads per rank for dataset builder
num_distributed_optimizer_instances: 1  # Number of Distributed Optimizer copies across Data Parallel domain.
num_experts: null  # Number of Experts in MoE (None means no MoE)
num_layers: null
num_layers_at_end_in_bf16: 1  # Number of layers at end to construct in bf16 when --first-last-layers-bf16 is enabled.
num_layers_at_start_in_bf16: 1  # Number of layers at start to construct in bf16 when --first-last-layers-bf16 is enabled.
num_layers_per_virtual_pipeline_stage: null  # Number of layers per virtual pipeline stage
num_query_groups: 1
num_virtual_stages_per_pipeline_rank: null  # Number of virtual pipeline stages per pipeline parallelism rank
num_workers: 2  # Dataloader number of workers.
object_storage_cache_path: null  # Path to cache index files when using s3 or msc dataloader
one_logger_async: false  # If set, forces one_logger to use async mode.
one_logger_project: megatron-lm  # The one-logger project name. Will ignore if --no-one-logger is set
one_logger_run_name: null  # The one-logger run name displayed. Will ignore if --no-one-logger is set
onnx_safe: null  # Use workarounds for known problems with Torch ONNX exporter
openai_gelu: false  # Use OpenAIs GeLU implementation. This optionshould not be used unless for backward compatibilityreasons.
optimizer: adam  # choices: ['adam', 'sgd'],Optimizer function
optimizer_cpu_offload: false  # Offload optimizer state to CPU
optimizer_offload_fraction: 1.0  # Ratio of optimizer state to offload to CPU
output_bert_embeddings: false  # Output Bert embeddings (via mean pooling) from model, rather than its binary head output or entire hidden batch.
overlap_cpu_optimizer_d2h_h2d: false  # Overlap CPU optimizer step, gradients D2H and updated parameters H2D.
overlap_grad_reduce: false  # If set, overlap DDP grad reduce.
overlap_p2p_comm: true  # overlap pipeline parallel communication with forward and backward chunks in 1F1B
overlap_p2p_comm_warmup_flush: false  # if set, overlap pipeline parallel communication in warmup and flush
overlap_param_gather: false  # If set, overlap param all-gather in distributed optimizer.
overlap_param_gather_with_optimizer_step: false  # If set, overlap param all-gather of first bucket with optimizer step.
override_opt_param_scheduler: false  # Reset the values of the scheduler (learning rate,warmup iterations, minimum learning rate, maximum number of iterations, and decay style from input arguments and ignore values from checkpoints. Notethat all the above values will be reset.
patch_dim: 16  # patch dimension
per_split_data_args_path: null  # Path to per-split-data-args. Instead of feeding `--(train|valid|test)-data-path` with weighted dataset, we pass in a file path from which we read those arguments. This is useful when the list of data is too big. Format is a json file with `train`, `valid, `test` keys
perform_initialization: true  # Do not perform initialization when building model, can reduce startup time when definitely loading from a checkpoint
pin_cpu_grads: true  # Disable pinning of CPU memory for gradients.
pin_cpu_params: true  # Disable pinning of CPU memory for parameters.
pipeline_model_parallel_comm_backend: null  # choices: ['nccl', 'ucc'],Select a communicator backend for pipeline parallel communication. If None, the default backend will be used.
pipeline_model_parallel_layout: null  # A string that describes a custom pipeline model parallel layout. e.g., "E|(t|)*3,m|m||L". E, L, t, m denotes embedding, loss, transformer decoder layer, and mtp layer, respectively. Stages are split by "|". Replicated stages or layers can be described with multiplication. Commas can be used cosmetically. Default None is not using this argument to set the layout.
pipeline_model_parallel_size: 1
pipeline_model_parallel_split_rank: null  # Rank where encoder and decoder should be split. Deprecated; use --encoder-pipeline-model-parallel-size instead.
position_embedding_type: learned_absolute  # choices: ['learned_absolute', 'rope', 'mrope', 'relative', 'none'],Position embedding type.
pretrained_checkpoint: null  # Directory containing a pretrained model checkpoint for finetuning.
profile: false  # Enable nsys profiling. When using this option, nsys options should be specified in commandline. An example nsys commandline is `nsys profile -s none -t nvtx,cuda -o <path/to/output_file> --force-overwrite true --capture-range=cudaProfilerApi --capture-range-end=stop`.
profile_ranks:  # Global ranks to profile.
- 0
profile_step_end: 12  # Global step to stop profiling.
profile_step_start: 10  # Global step to start profiling.
q_lora_rank: null  # Rank of Query tensor's low rank representation.
qk_head_dim: 128  # Dimension of the head in the QK projection. q_head_dim = qk_head_dim + qk_pos_emb_head_dim
qk_l2_norm: false  # Use llama 4 qk l2 norm
qk_layernorm: false  # Whether to layer normalize the q and k attention embeddings.
qk_pos_emb_head_dim: 64  # Dimension of the position embedding in the QK projection.
query_in_block_prob: 0.1  # Probability of keeping query in block for ICT dataset
rampup_batch_size: null  # Batch size ramp up with the following values:  --rampup-batch-size <start batch size>                       <batch size incerement>                       <ramp-up samples> For example:   --rampup-batch-size 16 8 300000 \    --global-batch-size 1024will start with global batch size 16 and over  (1024 - 16) / 8 = 126 intervals will increasethe batch size linearly to 1024. In each intervalwe will use approximately 300000 / 126 = 2380 samples.
recompute_activations: false  # recompute activation to allow for training with larger models, sequences, and batch sizes.
recompute_granularity: null  # choices: ['full', 'selective'],Checkpoint activations to allow for training with larger models, sequences, and batch sizes. It is supported at two granularities 1) full: whole transformer layer is recomputed, 2) selective: submodules set in --recompute-modules are recomputed, default is core_attn.
recompute_method: null  # choices: ['uniform', 'block'],1) uniform: uniformly divide the total number of Transformer layers and recompute the input activation of each divided chunk at specified granularity, 2) recompute the input activations of only a set number of individual Transformer layers per pipeline stage and do the rest without any recomputing at specified granularitydefault) do not apply activations recompute to any layers
recompute_modules: null  # The submodules to recompute. choices: "core_attn", "moe_act", "layernorm", "mla_up_proj", "mlp", "moe". default: ["core_attn"]."core_attn": recompute the core attention part of the transformer layer. "moe_act": recompute the MoE MLP activation function. "layernorm": recompute the input_layernorm and pre_mlp_layernorm. "mla_up_proj": recompute the MLA up projection and RoPE applying parts."mlp": recompute the dense MLP layer."moe": recompute the MoE layer."moe_act", "layernorm", and "mla_up_proj" use output-discarding checkpointing, "core_attn", "mlp", and "moe" uses normal checkpointing.
recompute_num_layers: null  # 1) uniform: the number of Transformer layers in each uniformly divided recompute unit, 2) block: the number of individual Transformer layers to recompute within each pipeline stage.
record_memory_history: false  # Record memory history in last rank.
relative_attention_max_distance: 128  # Maximum distance for relative position embeddings calculation.
relative_attention_num_buckets: 32  # Number of buckets for relative position embeddings.
replication: false  # If set, replication of local checkpoints is enabled. Needs to be enabled on all ranks.
replication_factor: 2  # Number of machines storing the replica of a given rank's data.
replication_jump: null  # Specifies `J`, the spacing between ranks storing replicas of a given rank's data. Replicas for rank `n` may be on ranks `n+J`, `n+2J`, ..., or `n-J`, `n-2J`, etc. This flag has an effect only if --replication is used. and must be consistent across all ranks.
rerun_mode: disabled  # choices: ['disabled', 'validate_results', 'report_stats'],Use re-run engine to validate results (default) or to emit stats on variability of computations due to non-deterministic algorithms.
reset_attention_mask: false  # Reset self attention maske after end-of-document token.
reset_position_ids: false  # Reset posistion ids after end-of-document token.
result_rejected_tracker_filename: null  # Optional name of file tracking `result_rejected` events.
retriever_report_topk_accuracies: []  # Which top-k accuracies to report (e.g. '1 5 20')
retriever_score_scaling: false  # Whether to scale retriever scores by inverse square root of hidden size
retriever_seq_length: 256  # Maximum sequence length for the biencoder model for retriever
retro_add_retriever: false  # Add a retriever to the transformer, for use in pretraining a Retro model.
retro_attention_gate: 1  # Gated cross attention.
retro_cyclic_train_iters: null  # Total number of iterations to train over all training runs. Note that either train-iters or train-samples should be provided.  # Set number of training iterations for cyclic Retro training.
retro_encoder_attention_dropout: 0.1  # Attention dropout for retrieval encoder.
retro_encoder_hidden_dropout: 0.1  # Hidden dropout for retrieval encoder.
retro_encoder_layers: 2  # Number of layers to use for the retrieval encoder.
retro_num_neighbors: 2  # Number of neighbors to retrieve during pretraining.
retro_num_retrieved_chunks: 2  # Number of chunks to retrieve from the retrieval database.
retro_project_dir: null  # Retro project directory, which contains the preprocessed data for pretraining. This directory is built during preprocessing (see tools/retro/README.md), and contains subdirectories for the chunk database and pretraining neighbors.
retro_verify_neighbor_count: true  # Skip verifying that len(GPT dataset) == len(saved neighbors).
reuse_grad_buf_for_mxfp8_param_ag: false  # If True, reuse the grad buffer for MXFP8 parameter all-gather.
rope_scaling_factor: 8.0  # Rope scaling factor in llama3.x models
rotary_base: 10000  # Base to use for rotary positional embeddings, default 10000
rotary_interleaved: false  # Use interleaved rotary embedding.
rotary_percent: 1.0  # Percent of rotary dimension to use, default 100%%
rotary_scaling_factor: 1.0  # Rotary scaling factor for the rotary embeddings.
rotary_seq_len_interpolation_factor: null  # Sequence length interpolation factor for rotary embeddings.
run_workload_inspector_server: false  # If set, enables workload inspector server for on-demand profiling.
sample_rate: 1.0  # sample rate for training data. Supposed to be 0  < sample_rate < 1
save: null
save_interval: null
scatter_gather_tensors_in_pipeline: true  # If not set, use scatter/gather to optimize communication of tensors in pipeline.
seed: 1234  # Random seed used for python, numpy, pytorch, and cuda.
seq_length: null
sequence_parallel: false  # Enable sequence parallel optimization.
sft: false  # Megatron SFT training
sft_tokenizer_prompt_format: nemotron-h-aligned  # SFT prompt format.
sgd_momentum: 0.9  # Momentum factor for sgd
short_seq_prob: 0.1  # Probability of producing a short sequence.
skip_train: false  # If set, bypass the training loop, optionally do evaluation for validation/test, and exit.
spec: null  # Specify the <module_location function_name> pair that returns a spec to customize a model, transformer block, or transformer layer, depending on the use case.To use local spec specify local as the argument.For more details, see the model class, `transformer_block.py`, or `transformer_layer.py`
split: null
squared_relu: false  # Use squared relu activation instead of default gelu
start_weight_decay: null  # Initial weight decay coefficient for L2 regularization.
straggler_ctrlr_port: 65535  # Port number to toggle StragglerDetector on/off at runtime
straggler_minmax_count: 1  # Number of ranks to report with high/low estimated throughput
suggested_communication_unit_size: null  # Specifies the number of elements to communicate at once during FSDP (Fully Sharded Data Parallel) operations. This flag also affects FSDP all-gather prefetch behavior. Setting a larger value increases the communication buffer size, while a smaller value disables prefetching and may degrade performance. Adjust this value based on your system's memory and performance requirements.
swiglu: false  # Use gated linear units and SiLU activation instead of default gelu
swin_backbone_type: tiny  # choices: ['tiny', 'base', 'h3'],pretraining objectives
symmetric_ar_type: null  # choices: ['two_shot', 'one_shot', 'multimem_all_reduce', None],What type of symmetric all reduce to use. The default is none which is no use of symetric memory
te_rng_tracker: false  # Use the Transformer Engine version of the random number generator. Required for CUDA graphs support.
tensor_model_parallel_size: 1
tensorboard_dir: null  # Write TensorBoard logs to this directory.
tensorboard_log_interval: 1  # Report to tensorboard interval.
tensorboard_queue_size: 1000  # Size of the tensorboard queue for pending events and summaries before one of the add calls forces a flush to disk.
test_data_path: null  # The weight and prefix list for an independent test dataset. Follows the same pattern rules as --data-path.
test_mode: false  # Run all real-time test alongside the experiment.
tiktoken_num_special_tokens: 1000  # Number of special tokens in tiktoken tokenizer
tiktoken_pattern: null  # Which tiktoken pattern to use. Options: [v1, v2]
tiktoken_special_tokens: null  # List of tiktoken special tokens, needs to have ["<unk>", "<s>", "</s>"]
timing_log_level: 0  # choices: range(0, 3),Granularity level to measure and report timing.    0: report only iteration time and make sure timing       does not introduce extra overhead.   1: report timing for operations that are executed       very limited times (basically once) during       each iteration (such as gradient all-reduce)    2: report timing for operations that migh be       executed numerous times during each iteration. Note that setting the level to 1 or 2 might cause increase in iteration time.
timing_log_option: minmax  # choices: ['max', 'minmax', 'all'],Options for logging timing:  max: report the max timing across all ranks  minmax: report min and max timings across all ranks  all: report timings of all ranks.
titles_data_path: null  # Path to titles dataset used for ICT
tokenizer_model: null  # Sentencepiece tokenizer model.
tokenizer_type: null  # choices: ['BertWordPieceLowerCase', 'BertWordPieceCase', 'GPT2BPETokenizer', 'SentencePieceTokenizer', 'GPTSentencePieceTokenizer', 'HuggingFaceTokenizer', 'Llama2Tokenizer', 'TikTokenizer', 'MultimodalTokenizer', 'NullTokenizer', 'NullMultimodalTokenizer', 'SFTTokenizer'],What type of tokenizer to use.
torch_fsdp2_reshard_after_forward: true  # Whether to reshard weights after forward pass when using PyTorch FSDP2. Set to enable FSDP ZeRO-2.
tp_comm_bootstrap_backend: nccl  # choices: ['nccl', 'mpi', 'gloo'],Set the bootstrapping backend of Tensor parallel communications.
tp_comm_bulk_dgrad: true  # Disables the All-Gather overlap with bprop activation gradient GEMM.
tp_comm_bulk_wgrad: true  # Disables the Reduce-Scatter overlap with bprop weight gradient GEMM.
tp_comm_overlap: false  # Enables the  overlap of Tensor parallel communication and GEMM kernels.
tp_comm_overlap_ag: true  # Disables the All-Gather overlap with GEMM by pipelining the GEMM and All-Gather.
tp_comm_overlap_cfg: null  # Config file when tp_comm_overlap is enabled.
tp_comm_overlap_rs: true  # Disables the Reduce-Scatter overlap with GEMM by pipelining the GEMM and Reduce-Scatter.
tp_comm_overlap_rs_dgrad: false  # Enables the Reduce-Scatter overlap with dgrad GEMM.
tp_comm_split_ag: true  # Disables the All-Gather overlap with fprop GEMM.
tp_comm_split_rs: true  # Disables the Reduce-Scatter overlap with fprop GEMM.
train_data_path: null  # The weight and prefix list for an independent train dataset. Follows the same pattern rules as --data-path.
train_iters: null
train_samples: null  # Total number of samples to train over all training runs. Note that either train-iters or train-samples should be provided.
train_sync_interval: null  # Training CPU-GPU synchronization interval, to ensure that CPU is not running too far ahead of GPU.
transformer_impl: transformer_engine  # choices: ['local', 'transformer_engine'],Which Transformer implementation to use.
untie_embeddings_and_output_weights: false  # Untie embeddings and output weights.
use_checkpoint_args: false  # Override model-related command-line arguments with arguments from checkpoint
use_checkpoint_opt_param_scheduler: false  # Use checkpoint to set the values of the scheduler (learning rate, warmup iterations, minimum learning rate, maximum number of iterations, and decay style from checkpoint and ignore input arguments.
use_cpu_initialization: null  # If set, initialize weights on the CPU. This eliminates init differences based on tensor parallelism.
use_custom_fsdp: false  # Use the Megatron FSDP code path in DDP.
use_dist_ckpt_deprecated: false  # Deprecated: see --ckpt-format.
use_distributed_optimizer: false  # Use distributed optimizer.
use_flash_attn: false  # use FlashAttention implementation of attention. https://arxiv.org/abs/2205.14135
use_legacy_models: false  # Use the legacy Megatron models, not Megatron-Core models.
use_mp_args_from_checkpoint_args: false  # Copy model parallelism command-line arguments from checkpoint
use_one_sent_docs: false  # Whether to use one sentence documents in ICT
use_persistent_ckpt_worker: false  # Enables a persitent checkpoint worker for async save
use_precision_aware_optimizer: false  # Use the precision-aware optimizer in TransformerEngine, which allows setting the main params and optimizer states to lower precision, such as fp16, bf16 and fp8.
use_pytorch_profiler: false  # Use the built-in pytorch profiler. Useful if you wish to view profiles in tensorboard.
use_ring_exchange_p2p: false  # If set, use custom-built ring exchange for p2p communications. Note that this option will require a custom built image that support ring-exchange p2p.
use_rope_scaling: false  # Apply rope scaling as used in llama3.x
use_rotary_position_embeddings: false  # Use rotary positional embeddings or not. Deprecated: use --position-embedding-type
use_sharp: false  # Required to enable SHARP communication.
use_tokenizer_model_from_checkpoint_args: true  # If set, do not use tokenizer model path from checkpoint
use_torch_fsdp2: false  # Use the torch FSDP2 implementation. FSDP2 has not been tested with pipeline parallelism, and may contain bugs.
use_torch_optimizer_for_cpu_offload: false  # Use torch.optim.Optimizer instead of Megatron's optimizer in optimizer cpu offload mode.
use_tp_pp_dp_mapping: false  # If set, distributed ranks initialize order is changed from tp-cp-ep-dp-pp to tp-cp-ep-pp-dp.
v_head_dim: 128  # Dimension of the head in the V projection.
valid_data_path: null  # The weight and prefix list for an independent validation dataset. Follows the same pattern rules as --data-path.
vision_backbone_type: vit  # choices: ['vit', 'mit', 'swin'],backbone types types
vision_pretraining: false  # flag to indicate vision pretraining
vision_pretraining_type: classify  # choices: ['classify', 'inpaint', 'dino'],pretraining objectives
vocab_extra_ids: 0  # Number of additional vocabulary tokens. They are used for span masking in the T5 model
vocab_file: null  # Path to the vocab file.
vocab_size: null  # Size of vocab before EOD or padding.
wandb_exp_name: ''  # The wandb experiment name.
wandb_project: ''  # The wandb project name. Ignore wandb by default.
wandb_save_dir: ''  # Path to save the wandb results locally.
warmup: null  # Old lr warmup argument, do not use. Use one of the--lr-warmup-* arguments above
weight_decay: 0.01
weight_decay_incr_style: constant  # choices: ['constant', 'linear', 'cosine'],Weight decay increment function.
wgrad_deferral_limit: 0  # Number of micro-batches for whichweight gradient computation of vocabulary projection is deferred, defaults to 0 whichmeans all the micro-batches are deferred. Invalid if `defer-embedding-wgrad-compute`is not set
yaml_cfg: null  # Config file to add additional arguments
