# @package _global_
defaults:
  - override /megatron: llama1.8b
  - _self_
  
megatron:
  split: "989,10,1"
  train_iters: 500
  lr_warmup_iters: 0
  save_interval: 1000000000
  save: ${output_dir}
  load: ${output_dir}
  tensorboard_dir: ${output_dir}/tensorboard